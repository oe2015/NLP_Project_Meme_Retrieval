{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel, SiglipModel, AutoProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, image_dir=\"images/train\", captions_file=\"memes-trainval.json\", transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        # Load the captions from the JSON file\n",
    "        with open(captions_file, 'r') as f:\n",
    "            self.captions_data = json.load(f)\n",
    "        \n",
    "        # Create a mapping from img_fname to the corresponding item in captions_data\n",
    "        self.captions_dict = {item['img_fname']: item for item in self.captions_data}\n",
    "\n",
    "        # List all images in the directory that have a corresponding entry in the captions_dict\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f in self.captions_dict]\n",
    "        print(f\"Total images found with captions: {len(self.image_files)}\")  # For debugging\n",
    "\n",
    "        # Use provided transform or a default transform if not specified\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize to a consistent size\n",
    "            transforms.ToTensor()  # Converts the image to a tensor with values in [0, 1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fname = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_fname)\n",
    "\n",
    "        # Open the image\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            # Convert the image to RGB format if not already\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "\n",
    "            # Apply the transformation\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Get the corresponding captions\n",
    "            captions_info = self.captions_dict[img_fname]\n",
    "            img_captions = \" \".join(captions_info['img_captions']) if captions_info['img_captions'] else \"\"\n",
    "            meme_captions = \" \".join(captions_info['meme_captions']) if captions_info['meme_captions'] else \"\"\n",
    "\n",
    "            return image, img_captions, meme_captions  # Return the image tensor and the captions\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_fname}: {e}\")\n",
    "            return None  # Return None if there is an error\n",
    "        \n",
    "\n",
    "# Contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, image_embeds, caption_embeds):\n",
    "        # Compute logits (dot product of image and caption embeddings)\n",
    "        logits = (image_embeds @ caption_embeds.T) / self.temperature\n",
    "        labels = torch.arange(len(logits)).to(logits.device)\n",
    "        return self.loss_fn(logits, labels)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SigLIPLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t_prime = nn.Parameter(torch.tensor(0.07))  # Learnable temperature\n",
    "        self.b = nn.Parameter(torch.tensor(0.0))        # Learnable bias\n",
    "\n",
    "    def forward(self, image_embeds, caption_embeds):\n",
    "        # Step 1: Normalize the embeddings\n",
    "        zimg = F.normalize(image_embeds, p=2, dim=1)\n",
    "        ztxt = F.normalize(caption_embeds, p=2, dim=1)\n",
    "\n",
    "        # Step 2: Compute logits\n",
    "        t = torch.exp(self.t_prime)\n",
    "        logits = zimg @ ztxt.T * t + self.b  # Pairwise similarity logits\n",
    "\n",
    "        # Step 3: Construct labels\n",
    "        n = image_embeds.size(0)  # Batch size\n",
    "        labels = 2 * torch.eye(n, device=logits.device) - torch.ones((n, n), device=logits.device)\n",
    "\n",
    "        # Step 4: Compute Sigmoid Loss\n",
    "        loss = -torch.mean(F.logsigmoid(labels * logits))  # Average loss over all pairs\n",
    "        return loss\n",
    "\n",
    "\n",
    "# # Evaluation (Example Query)\n",
    "# model.eval()\n",
    "# query = \"Spider Man making a suggestion\"  # Example query\n",
    "# inputs = processor(text=[query], return_tensors=\"pt\", padding=True).to(device)\n",
    "# with torch.no_grad():\n",
    "#     query_embeds = model.get_text_features(**inputs)\n",
    "\n",
    "# Now, you can use cosine similarity or nearest neighbors to retrieve top-k meme images\n",
    "\n",
    "class BinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, image_embeds, caption_embeds):\n",
    "        # Compute pairwise similarity as logits\n",
    "        logits = image_embeds @ caption_embeds.T\n",
    "        \n",
    "        # Create binary targets (1 for matching pairs, 0 for others)\n",
    "        targets = torch.eye(len(logits), device=logits.device)\n",
    "        \n",
    "        # Flatten logits and targets for BCE loss\n",
    "        logits = logits.flatten()\n",
    "        targets = targets.flatten()\n",
    "        \n",
    "        return self.loss_fn(logits, targets)\n",
    "\n",
    "# Replace the contrastive loss with BinaryCrossEntropyLoss\n",
    "contrastive_loss = BinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load the CLIP model and processor\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # Call this before model initialization and training\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model = SiglipModel.from_pretrained(\"google/siglip-base-patch16-224\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found with captions: 5430\n",
      "Total images found with captions: 523\n",
      "Training samples: 4887\n",
      "Validation samples: 543\n",
      "Test samples: 523\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x14ac081dad00>\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load your dataset\n",
    "import json\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# Load your dataset from a JSON file\n",
    "with open('memes-trainval.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # Call this before model initialization and training\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out empty entries\n",
    "    batch = [item for item in batch if item]\n",
    "    if len(batch) == 0:\n",
    "        print(\"empty batch?\")\n",
    "\n",
    "    return default_collate(batch)\n",
    "\n",
    "# Update your DataLoader\n",
    "train_dataset = MemeDataset(image_dir=\"images/train\", captions_file=\"memes-trainval.json\")\n",
    "\n",
    "test_dataset = MemeDataset(image_dir=\"images/test\", captions_file=\"memes-test.json\")\n",
    "\n",
    "# Calculate the number of samples for training and validation\n",
    "train_size = int(0.9 * len(train_dataset))  # 90% for training\n",
    "val_size = len(train_dataset) - train_size  # Remaining 10% for validation\n",
    "\n",
    "# Split the dataset\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training samples: {len(train_subset)}\")\n",
    "print(f\"Validation samples: {len(val_subset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def validate_model(model, processor, val_loader, device, top_k=5):\n",
    "    model.eval()\n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "    ground_truth = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, img_captions, meme_captions in val_loader:  # Ignore meme_captions\n",
    "            # Move images to the GPU\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Generate embeddings for the images\n",
    "            image_inputs = processor.image_processor(images=images, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            image_outputs = model.get_image_features(**image_inputs)\n",
    "            image_embeddings.append(image_outputs.cpu().numpy())\n",
    "\n",
    "            # Generate embeddings for the image captions\n",
    "            img_captions = list(img_captions)\n",
    "            img_caption_inputs = processor.tokenizer(img_captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "            text_outputs = model.get_text_features(**img_caption_inputs)\n",
    "            text_embeddings.append(text_outputs.cpu().numpy())\n",
    "\n",
    "            # Append ground truth indices\n",
    "            batch_size = len(img_captions)\n",
    "            ground_truth.extend(range(len(ground_truth), len(ground_truth) + batch_size))\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    image_embeddings = np.concatenate(image_embeddings, axis=0)\n",
    "    text_embeddings = np.concatenate(text_embeddings, axis=0)\n",
    "    print(\"Image Embeddings Shape:\", image_embeddings.shape)\n",
    "    print(\"Text Embeddings Shape:\", text_embeddings.shape)\n",
    "\n",
    "    # Calculate cosine similarities between text and image embeddings\n",
    "    similarities = cosine_similarity(text_embeddings, image_embeddings)\n",
    "    print(\"Similarities Shape:\", similarities.shape)\n",
    "\n",
    "    # Evaluate top-K accuracy\n",
    "    top_k_accuracy = 0\n",
    "    for idx, sim in enumerate(similarities):\n",
    "        top_k_indices = np.argsort(sim)[-top_k:][::-1]  # Get top-K indices\n",
    "\n",
    "        # Check if the ground truth index is in the top-K\n",
    "        if ground_truth[idx] in top_k_indices:\n",
    "            top_k_accuracy += 1\n",
    "\n",
    "    # Calculate the percentage of correct top-K predictions\n",
    "    top_k_accuracy /= len(ground_truth)\n",
    "    print(f\"Top-{top_k} Accuracy: {top_k_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return top_k_accuracy\n",
    "\n",
    "# Run validation\n",
    "# val_accuracy = validate_model(model, processor, val_loader, device, top_k=5)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Function to save the model checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1} with validation loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [20], Progress: 2.95%\n",
      "Epoch [1/50], Batch [40], Progress: 5.89%\n",
      "Epoch [1/50], Batch [60], Progress: 8.84%\n",
      "Epoch [1/50], Batch [80], Progress: 11.79%\n",
      "Epoch [1/50], Batch [100], Progress: 14.73%\n",
      "Epoch [1/50], Batch [120], Progress: 17.68%\n",
      "Epoch [1/50], Batch [140], Progress: 20.63%\n",
      "Image Embeddings Shape: (523, 768)\n",
      "Text Embeddings Shape: (523, 768)\n",
      "Similarities Shape: (523, 523)\n",
      "Top-5 Accuracy: 20.46%\n",
      "Checkpoint saved at epoch 1 with validation loss: 0.2046\n",
      "Epoch 1, Loss: 0.6730559499435176\n",
      "Epoch [2/50], Batch [20], Progress: 2.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar.el-herraoui/.conda/envs/ai701/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Batch [40], Progress: 5.89%\n",
      "Epoch [2/50], Batch [60], Progress: 8.84%\n",
      "Epoch [2/50], Batch [80], Progress: 11.79%\n",
      "Epoch [2/50], Batch [100], Progress: 14.73%\n",
      "Epoch [2/50], Batch [120], Progress: 17.68%\n",
      "Epoch [2/50], Batch [140], Progress: 20.63%\n",
      "Image Embeddings Shape: (523, 768)\n",
      "Text Embeddings Shape: (523, 768)\n",
      "Similarities Shape: (523, 523)\n",
      "Top-5 Accuracy: 20.08%\n",
      "Epoch 2, Loss: 0.668632255660163\n",
      "Epoch [3/50], Batch [20], Progress: 2.95%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, img_captions, meme_captions \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Ignore meme_captions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Move images to the GPU\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Process images with img_captions\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m, in \u001b[0;36mMemeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Apply the transformation\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Get the corresponding captions\u001b[39;00m\n\u001b[1;32m     53\u001b[0m captions_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptions_dict[img_fname]\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the best validation loss as infinity\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# %%\n",
    "# Define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, weight_decay=1e-4)\n",
    "contrastive_loss = SigLIPLoss()\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(50):  # Number of epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    for images, img_captions, meme_captions in train_loader:  # Ignore meme_captions\n",
    "        # Move images to the GPU\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Process images with img_captions\n",
    "        img_captions = list(img_captions)\n",
    "        img_caption_inputs = processor.tokenizer(img_captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "        image_inputs = processor.image_processor(images=images, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "\n",
    "        # Get embeddings for images with img_captions\n",
    "        inputs = {\n",
    "            \"pixel_values\": image_inputs[\"pixel_values\"].to(device),  # Ensure on GPU\n",
    "            \"input_ids\": img_caption_inputs[\"input_ids\"].to(device),  # Ensure on GPU\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        img_caption_embeds = outputs.text_embeds\n",
    "\n",
    "        # Compute contrastive loss for the image-caption pair\n",
    "        loss = contrastive_loss(outputs.image_embeds, img_caption_embeds)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_count += 1\n",
    "        # Print progress every 20 batches\n",
    "        if batch_count % 20 == 0:\n",
    "            progress = (batch_count * 8) / 5430 * 100  # Calculate progress percentage\n",
    "            print(f\"Epoch [{epoch+1}/50], Batch [{batch_count}], Progress: {progress:.2f}%\")\n",
    "\n",
    "    # Perform validation\n",
    "    model.eval()\n",
    "    val_accuracy = validate_model(model, processor, test_loader, device, top_k=5)\n",
    "\n",
    "    # Save the best checkpoint if the validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        checkpoint_path = f\"best_checkpoint_meme_captions_new_last_epoch_new_siglip_{epoch + 1}.pt\"\n",
    "        save_checkpoint(model, optimizer, epoch, best_val_accuracy, checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904397705544933\n",
      "Image Embeddings Shape: (523, 768)\n",
      "Text Embeddings Shape: (523, 768)\n",
      "Similarities Shape: (523, 523)\n",
      "Top-10 Accuracy: 26.20%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#test set\n",
    "# %%\n",
    "# Load the saved model checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Checkpoint loaded from '{checkpoint_path}' at epoch {checkpoint['epoch'] + 1}\")\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "\n",
    "# %%\n",
    "# Path to the best checkpoint\n",
    "checkpoint_path = \"best_checkpoint_meme_captions_new_last_epoch_new_1.pt\"  # Replace <epoch_number> with the actual number\n",
    "\n",
    "# Load the best checkpoint\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7)  # Re-initialize optimizer for checkpoint loading\n",
    "# epoch, best_loss = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "print(best_loss)\n",
    "# Evaluate on the test set\n",
    "test_accuracy = validate_model(model, processor, test_loader, device, top_k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
